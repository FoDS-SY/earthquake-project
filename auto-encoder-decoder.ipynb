{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc20364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import glob\n",
    "import math\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, ZeroPadding2D, Cropping2D, BatchNormalization, Flatten, Bidirectional, Dropout, Reshape,InputSpec\n",
    "from keras import regularizers\n",
    "from tensorflow.python.keras.engine.base_layer import Layer\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard, CSVLogger\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras.utils import plot_model\n",
    "import random\n",
    "from math import *\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib.pyplot import savefig\n",
    "from tensorflow.python.ops import math_ops\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, FeatureAgglomeration, DBSCAN, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import struct\n",
    "import matplotlib\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac39b1",
   "metadata": {},
   "source": [
    "length  of input \n",
    "length of latent representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5d20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataname = './data_processed.npy'\n",
    "data_processed = np.load(train_dataname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d42206",
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w,c = np.shape(data_processed)\n",
    "data = np.empty((h,w,c,1))\n",
    "for i in range(c):\n",
    "    data[:,:,i,0] = data_processed[:,:,i]\n",
    "data_input = Input(shape=(h, w, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5152494",
   "metadata": {},
   "source": [
    "## auto encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9f075bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth      = 8\n",
    "strides    = 1\n",
    "activation = 'relu'\n",
    "# ReLU initial weights\n",
    "kernel_initializer='glorot_uniform'\n",
    "latent_dim = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3062d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "layer1 = Conv2D(depth*2**0, (5,5), strides=strides, activation=activation, kernel_initializer=kernel_initializer, padding='same') (img_input)\n",
    "layer2 = Conv2D(depth*2**1, (5,5), strides=strides, activation=activation, kernel_initializer=kernel_initializer, padding='same') (e)\n",
    "layer3 = Conv2D(depth*2**2, (3,3), strides=strides, activation=activation, kernel_initializer=kernel_initializer, padding='same') (e)\n",
    "layer4 = Conv2D(depth*2**3, (3,3), strides=strides, activation=activation, kernel_initializer=kernel_initializer, padding='same')(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode\n",
    "decoder_layer1 = Dense(np.prod(shape_before_flattening[1:]), activation='relu')(encoded)\n",
    "decoder_layer2 = Reshape(shape_before_flattening[1:])(d)\n",
    "decoder_layer3 = Conv2DTranspose(depth*2**2, (3,3), strides=strides, activation=activation, kernel_initializer=kernel_initializer, padding='same')(d)\n",
    "decoder_layer4= Conv2DTranspose(depth*2**1, (5,5), strides=strides, activation=activation, kernel_initializer=kernel_initializer, padding='same')(d)\n",
    "decoder_layer5 = Conv2DTranspose(depth*2**0, (5,5), strides=strides, activation=activation, kernel_initializer=kernel_initializer, padding='same')(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ec469",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=img_input, outputs=encoded, name='encoder')\n",
    "autoencoder = Model(inputs=img_input, outputs=decoded, name='autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a309bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001 \n",
    "n_epochs = 500  \n",
    "batch_size = 1024 \n",
    "\n",
    "logger_fname = 'HistoryLog_LearningCurve.csv'.format(date_name)\n",
    "csv_logger = CSVLogger(logger_fname)\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1,\n",
    "                           mode='min', restore_best_weights=True)\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(lr=learning_rate)             \n",
    "loss = 'mse'                              \n",
    "\n",
    "encoder.compile(loss=loss,optimizer=optim)\n",
    "autoencoder.compile(loss=loss,\n",
    "                  optimizer=optim,\n",
    "                  metrics=[metrics.mae])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(data, data, batch_size=batch_size, epochs=n_epochs, callbacks=[csv_logger, early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184d0a6",
   "metadata": {},
   "source": [
    "## GMM fitting part\n",
    "\n",
    "- get latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96473fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.distributions import MultivariateNormalFullCovariance\n",
    "MVNFC = MultivariateNormalFullCovariance\n",
    "\n",
    "\n",
    "def gmm(mu, cov, tau, sx_proj, n_clusters=None, gmm_type='natural',\n",
    "        trainable=True, cov_diag=None):\n",
    "    \"\"\"Gaussian mixture clustering.\"\"\"\n",
    "    if trainable is True:\n",
    "        log_tau = tf.log(tf.nn.softmax(tau))\n",
    "        eps = tf.eye(2)\n",
    "        d = cov_diag\n",
    "        gm = [MVNFC(mu[c], tf.nn.elu(d[c]) * np.eye(2) +\n",
    "                    tf.matmul(cov[c], tf.transpose(cov[c])) + eps)\n",
    "              for c in range(n_clusters)]\n",
    "    else:\n",
    "        n_clusters = cov.get_shape().as_list()[0]\n",
    "        log_tau = tf.log(tau)\n",
    "        gm = [MVNFC(mu[c], cov[c]) for c in range(n_clusters)]\n",
    "    log_p = [gm[c].log_prob(sx_proj) for c in range(n_clusters)]\n",
    "    cat = tf.stack([log_tau[c] + log_p[c] for c in range(n_clusters)], 1)\n",
    "\n",
    "    # Choose you loss\n",
    "    if gmm_type == 'natural':\n",
    "        q = tf.reduce_logsumexp(cat, axis=1)\n",
    "        loss = - tf.reduce_mean(q)\n",
    "    else:\n",
    "        y = tf.argmax(cat, axis=1)\n",
    "        y_un, _ = tf.unique(y)\n",
    "        q = tf.reduce_sum(tf.stop_gradient(tf.nn.softmax(cat)) * cat, 1)\n",
    "        loss = tf.map_fn(\n",
    "            lambda c: tf.reduce_sum(\n",
    "                q * tf.cast(tf.equal(y, tf.ones_like(y) * c),\n",
    "                            tf.float32)) /\n",
    "            tf.reduce_sum(\n",
    "                tf.cast(tf.equal(y, tf.ones_like(y) * c), tf.float32)),\n",
    "            y_un, dtype=tf.float32) * 1e3\n",
    "\n",
    "    return loss, cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f782906",
   "metadata": {},
   "source": [
    "## KMEANS fitting part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(2, 10, 1):\n",
    "    kmeans_model_test = KMeans(n_clusters=i).fit(enc_train)\n",
    "    inertia.append(kmeans_model_test.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871895a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=100) # run kmeans with n_clusters, run 100 initializations to ensure accuracy\n",
    "\n",
    "labels = kmeans.fit_predict(enc_train)             # get initial assignments\n",
    "\n",
    "labels_last = np.copy(labels)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
